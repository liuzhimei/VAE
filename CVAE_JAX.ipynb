{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa36f9d-ccc8-42e6-bd96-f67eec9047f0",
   "metadata": {},
   "source": [
    "# Conditional Variational Autoencoder with JAX \n",
    "Trained with MNIST images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98e488b-e180-4427-92ac-503b532003e2",
   "metadata": {},
   "source": [
    "## How the CVAE learns the connection between images and labels:\n",
    "1. Conditional Encoding and Latent Space Structuring\n",
    "\n",
    "    In the encoder, both the image $x$ and label $c$ (converted to a one-hot encoded vector) are concatenated and passed through the network to learn a latent representation $z$. By including $c$, the encoder learns to structure the latent space based on this additional label information, enabling it to differentiate representations by class or label.\n",
    "\n",
    "    The encoder learns to map $(x,c)$ to a latent distribution $q(z|x,c)$, capturing data variations within each class separately.\n",
    "\n",
    "    This structuring means that, for example, latent representations for images of a digit \"3\" will be more clustered in one part of the space, while \"7\" might be in another, with the label $c$ guiding this separation.\n",
    "\n",
    "2. Conditional Decoding\n",
    "\n",
    "   During decoding, the CVAE uses both the latent variable $z$ and the label $c$ to reconstruct the input as $p(x|z,c)$. This conditioning ensures that generated samples are aligned with the given label $c$, allowing the model to output images that resemble the conditioned label. Essentially, it gives control over the generation process, making it possible to generate specific types of data (e.g., MNIST digits corresponding to \"3\" or \"7\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288905e1-1e61-49d8-88b6-dace5a88a204",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e9687808-279b-4e25-87e2-39aa28f62906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aed8e9-a761-4d3f-a51c-1e9f12539570",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d1cdb9-1497-402e-bb98-c5ecbd346534",
   "metadata": {},
   "source": [
    "## Onehot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f774e1d-bb9c-4764-9ab5-b3f2dae2b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(x, max_dim):\n",
    "    \"\"\"Convert labels to one-hot encoding.\"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    vector = jnp.zeros((batch_size, max_dim))\n",
    "    one_hot = vector.at[jnp.arange(batch_size), x].set(1)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edf5514-67bc-4e2e-b0d6-d12db4ca588a",
   "metadata": {},
   "source": [
    "## Build encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5c0603d-b3ce-4dde-a96f-7431ca928f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    hidden_size: int\n",
    "    latent_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.linear = nn.Dense(self.hidden_size)\n",
    "        self.mu = nn.Dense(self.latent_size)\n",
    "        self.sigma = nn.Dense(self.latent_size)\n",
    "        \n",
    "        # # Convolutional layers\n",
    "        # self.conv1 = nn.Conv(features=32, kernel_size=(3, 3), strides=(2, 2))  # output: 14x14x32\n",
    "        # self.conv2 = nn.Conv(features=64, kernel_size=(3, 3), strides=(2, 2))  # output: 7x7x64\n",
    "\n",
    "        # # Fully connected layers\n",
    "        # self.fc = nn.Dense(self.hidden_size)\n",
    "        # self.mu = nn.Dense(self.latent_size)\n",
    "        # self.sigma = nn.Dense(self.latent_size)\n",
    "\n",
    "    # def __call__(self, x):\n",
    "    #     x = nn.relu(self.conv1(x))\n",
    "    #     x = nn.relu(self.conv2(x))\n",
    "    #     x = jnp.reshape(x, (x.shape[0], -1))  # Flatten the 2D feature maps manually\n",
    "    #     x = nn.relu(self.fc(x))\n",
    "    #     mu = self.mu(x)\n",
    "    #     sigma = self.sigma(x)\n",
    "    #     return mu, sigma\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # print(\"Input to Encoder:\", x.shape)  # Print input shape\n",
    "        x = nn.relu(self.linear(x))\n",
    "        # print(\"After Linear Layer:\", x.shape)  # Print shape after linear layer\n",
    "        mu = self.mu(x)\n",
    "        # print(\"Mu Shape:\", mu.shape)  # Print mu shape\n",
    "        sigma = self.sigma(x)\n",
    "        # print(\"Sigma Shape:\", sigma.shape)  # Print sigma shape\n",
    "        return mu, sigma\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c05919c-18d8-440c-a297-89e5a0547267",
   "metadata": {},
   "source": [
    "## Build decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b473f264-9d85-4215-bb37-25337537b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    hidden_size: int\n",
    "    output_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.linear1 = nn.Dense(self.hidden_size)\n",
    "        self.linear2 = nn.Dense(self.output_size)\n",
    "\n",
    "        # # Fully connected layers to expand from latent space to feature map\n",
    "        # self.fc = nn.Dense(self.hidden_size)\n",
    "        # self.fc_reshape = nn.Dense(7 * 7 * 64)\n",
    "        \n",
    "        # # Transpose Convolutional layers\n",
    "        # self.fc_reshape = nn.Dense(7 * 7 * 64)\n",
    "        # self.convT1 = nn.ConvTranspose(features=64, kernel_size=(3, 3), strides=(2, 2), padding='SAME')  # upsample to 14x14\n",
    "        # self.convT2 = nn.ConvTranspose(features=32, kernel_size=(3, 3), strides=(2, 2), padding='SAME')  # upsample to 28x28\n",
    "        # self.convT3 = nn.ConvTranspose(features=1, kernel_size=(3, 3), strides=(1, 1), padding='SAME')  # final output\n",
    "\n",
    "\n",
    "    # def __call__(self, x):\n",
    "        # x = nn.relu(self.fc(x))\n",
    "        # x = nn.relu(self.fc_reshape(x))\n",
    "        # x = jnp.reshape(x, (-1, 7, 7, 64))  # Reshape to a 2D feature map for transposed convolutions\n",
    "        # x = nn.relu(self.convT1(x))\n",
    "        # x = nn.relu(self.convT2(x))\n",
    "        # x = nn.sigmoid(self.convT3(x)).reshape((-1, 28 * 28))  # Flatten for binary cross-entropy\n",
    "        # return x\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # print(\"Input to Decoder:\", x.shape)  # Print input shape\n",
    "        x = nn.relu(self.linear1(x))\n",
    "        # print(\"After First Linear Layer:\", x.shape)  # Print shape after first linear layer\n",
    "        x = nn.sigmoid(self.linear2(x))\n",
    "        # print(\"Output Shape:\", x.shape)  # Print output shape\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b072bd8-f37b-4975-8f37-c7aa95e17faa",
   "metadata": {},
   "source": [
    "## Combine encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ea5465f2-11a5-4d4f-b633-b0c4d5a1ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    input_size: int\n",
    "    output_size: int\n",
    "    condition_size: int\n",
    "    latent_size: int\n",
    "    hidden_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.encoder = Encoder(hidden_size=self.hidden_size, latent_size=self.latent_size)\n",
    "        self.decoder = Decoder(hidden_size=self.hidden_size, output_size=self.output_size)\n",
    "\n",
    "    def __call__(self, x, c):\n",
    "        # print(\"Shape of x:\", x.shape)\n",
    "        # print(\"Shape of c:\", c.shape)\n",
    "        x = jnp.concatenate((x, c), axis=1) # Concatenate the input x and condition c\n",
    "        # print(\"Shape after concatenating x and c:\", x.shape)  # Print input shape to CVAE\n",
    "        mu, sigma = self.encoder(x)\n",
    "        # print(\"Mu Shape:\", mu.shape)  # Print shape of mu\n",
    "        # print(\"Sigma Shape:\", sigma.shape)  # Print shape of sigma\n",
    "\n",
    "        # Sample from standard normal distribution\n",
    "        eps = jax.random.normal(jax.random.PRNGKey(0), sigma.shape)\n",
    "        z = mu + eps * jnp.exp(0.5 * sigma)\n",
    "        z_cond = jnp.concatenate((z, c), axis=1)\n",
    "        # print(\"Input to Decoder:\", z.shape)  # Print input shape to decoder\n",
    "\n",
    "        recon_x = self.decoder(z_cond)\n",
    "        return recon_x, mu, sigma\n",
    "\n",
    "    def decode(self, z, c):\n",
    "        \"\"\"Decode from latent space given a condition.\"\"\"\n",
    "        z = jnp.concatenate((z, c), axis=1)  # Concatenate latent vector and condition\n",
    "        return self.decoder(z)  # Use decoder to reconstruct the image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ccb0c-3171-4e68-9b93-bd1a1298b76a",
   "metadata": {},
   "source": [
    "## Define loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0258ce1-1bad-4379-a9df-e941753770c1",
   "metadata": {},
   "source": [
    "The CVAE’s loss function has the same terms as the VAE, combining the **reconstruction loss** (likelihood of data given latent and condition) and the **KL divergence** (difference between learned and prior distributions). However, both terms now incorporate the condition $c$ as a context, which refines how the loss shapes the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f2f997b-fbd1-407c-8c75-7502bae63b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, mu, sigma):\n",
    "    \"\"\"Compute the loss for the CVAE.\"\"\"\n",
    "    loss_re = optax.sigmoid_binary_cross_entropy(recon_x, x).sum()\n",
    "    loss_norm = -0.5 * jnp.sum(1 + sigma - jnp.square(mu) - jnp.exp(sigma))\n",
    "    return loss_re + loss_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3569cd-4259-4ce6-b673-dd913c943a92",
   "metadata": {},
   "source": [
    "# Hyperparameters and model construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd9d3a9-9bf7-4767-8afc-7c9e3e762444",
   "metadata": {},
   "source": [
    "## Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8a7ced0f-4b18-49a7-92ca-ccf0430ff0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 16  # latent space dimension\n",
    "hidden_size = 128  # hidden layer dimension\n",
    "input_size = output_size = 28 * 28 \n",
    "condition_size = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c5e076-7991-417a-a649-ab58430ede4a",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "be3b4bd8-e703-4769-a9b6-fef26ca49182",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32 \n",
    "learning_rate = 3e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c2b969-03dd-4cfb-ba01-893613034f26",
   "metadata": {},
   "source": [
    "## Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "44e48382-5dd6-466d-bd35-fbcd531d4fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "model = CVAE(input_size, output_size, condition_size, latent_size, hidden_size)\n",
    "# Update the input shape to include the condition size\n",
    "input_shape = (batch_size, input_size)  # Correct shape\n",
    "params = model.init(jax.random.PRNGKey(0), jnp.ones(input_shape), jnp.ones((batch_size, condition_size)))\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7628d2d4-f9c1-4ec2-a7bc-4c862a340545",
   "metadata": {},
   "source": [
    "# Load dataset and visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe1a4141-7589-4a85-a0c7-4844dbcd795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using PyTorch\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "05e6c558-ccc6-4ed4-b501-466796417167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zx/j5d_p2_s2wz05zg3b78717100000gn/T/ipykernel_39193/679139505.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_tensor = torch.tensor(batch[0])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    # Convert the list to a tensor\n",
    "    data_tensor = torch.tensor(batch[0])\n",
    "    print(data_tensor.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "164e7764-1e04-4a59-a5e2-a9c75978544f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training set: 60000\n",
      "Number of samples in test dataset: 10000\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Check the length of the dataset\n",
    "print(f\"Number of samples in training set: {len(train_dataset)}\")\n",
    "print(f\"Number of samples in test dataset: {len(test_dataset)}\")\n",
    "print(train_dataset[60][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc488584-70f7-49fe-b679-19687877e7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28])\n",
      "Image data type: torch.float32\n",
      "Label: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def inspect_train_set(train_dataset):\n",
    "    '''Function to inspect the train_set'''\n",
    "\n",
    "    # Get the first sample (image, label)\n",
    "    sample_image, sample_label = train_dataset[60]\n",
    "\n",
    "    # Check the shape and type of the image\n",
    "    print(f\"Image shape: {sample_image.shape}\")\n",
    "    print(f\"Image data type: {sample_image.dtype}\")\n",
    "    print(f\"Label: {sample_label}\")\n",
    "\n",
    "    # Visualize the first image\n",
    "    plt.imshow(sample_image.squeeze(), cmap='gray')\n",
    "    plt.title(f\"Label: {sample_label}\")\n",
    "    plt.show()\n",
    "\n",
    "# Inspect the train_set\n",
    "inspect_train_set(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "05b4762f-b6b4-4280-b7f7-c53d781ad5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for imgs, lbls in tqdm(train_loader):\n",
    "#     # Flatten images to shape (batch_size, 784)\n",
    "#     imgs = imgs.view(imgs.shape[0], -1)  \n",
    "#     print(\"Shape of imgs:\", imgs.shape)  # Should be (batch_size, 784)\n",
    "    \n",
    "#     # One-hot encode the labels\n",
    "#     lbls = onehot(lbls.numpy(), condition_size)  # Convert to one-hot encoding\n",
    "#     print(\"Shape of lbls:\", lbls.shape)  # Should be (batch_size, 10)\n",
    "\n",
    "#     # Convert to JAX arrays\n",
    "#     imgs = jnp.array(imgs.numpy())  # Shape (batch_size, 784)\n",
    "#     lbls = jnp.array(lbls)  # Shape (batch_size, 10)\n",
    "\n",
    "#     # Concatenate images and labels for encoder input\n",
    "#     inputs = jnp.concatenate((imgs, lbls), axis=1)  # Shape (batch_size, 794)\n",
    "#     print(\"Shape of inputs:\", inputs.shape)  # Should be (batch_size, 794)\n",
    "    \n",
    "#     params = model.init(jax.random.PRNGKey(0), inputs, lbls)  # Initialize parameters\n",
    "\n",
    "#     # Call the model with both inputs\n",
    "#     recon_x, mu, sigma = model.apply(params, inputs, lbls)  # Pass lbls as the second argument\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c57b1-d2b0-4da7-9520-c621d24ab7c6",
   "metadata": {},
   "source": [
    "# Train the model and visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cbdd2eec-7e86-45bd-a7e7-62b600bf7e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:13<00:00, 25.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0| Train Loss:  17351.312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:04<00:00, 75.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0| Test Loss:  17203.549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:14<00:00, 25.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1| Train Loss:  17205.127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:04<00:00, 77.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1| Test Loss:  17132.434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:12<00:00, 25.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2| Train Loss:  17157.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:04<00:00, 77.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2| Test Loss:  17099.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:13<00:00, 25.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3| Train Loss:  17131.828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:04<00:00, 76.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3| Test Loss:  17075.016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:12<00:00, 25.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4| Train Loss:  17113.395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:04<00:00, 68.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4| Test Loss:  17066.365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:11<00:00, 26.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5| Train Loss:  17098.943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:04<00:00, 76.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5| Test Loss:  17044.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:12<00:00, 25.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6| Train Loss:  17084.398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:04<00:00, 77.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6| Test Loss:  17032.268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:11<00:00, 26.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7| Train Loss:  17071.371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:04<00:00, 72.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7| Test Loss:  17021.742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:12<00:00, 25.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8| Train Loss:  17061.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:04<00:00, 75.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8| Test Loss:  17004.184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:13<00:00, 25.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9| Train Loss:  17046.518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:04<00:00, 76.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9| Test Loss:  16998.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for imgs, lbls in tqdm(train_loader):\n",
    "        imgs = imgs.view(imgs.shape[0], input_size)  # Flatten the images\n",
    "\n",
    "        # print(f\"input of onehot function: {lbls.numpy()}\")\n",
    "        lbls = onehot(lbls.numpy(), condition_size)  # Convert labels to one-hot encoding\n",
    "        # print(f\"output of onehot function: {lbls}\")\n",
    "\n",
    "        # Move data to JAX compatible format\n",
    "        imgs = jnp.array(imgs.numpy())  # Convert to JAX array\n",
    "        lbls = jnp.array(lbls)  # Convert to JAX array\n",
    "\n",
    "        def loss_fn_wrapper(params):\n",
    "            recon_x, mu, sigma = model.apply(params, imgs, lbls)\n",
    "            return loss_fn(recon_x, imgs, mu, sigma)\n",
    "\n",
    "        loss, grads = jax.value_and_grad(loss_fn_wrapper)(params)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "\n",
    "        train_loss += loss\n",
    "\n",
    "    print(f'epoch: {epoch}| Train Loss: ', train_loss / len(train_loader))\n",
    "\n",
    "    # Evaluation\n",
    "    test_loss = 0\n",
    "    for imgs, lbls in tqdm(test_loader):\n",
    "        imgs = imgs.view(imgs.shape[0], input_size)\n",
    "        lbls = onehot(lbls.numpy(), condition_size)\n",
    "        \n",
    "        imgs = jnp.array(imgs.numpy())\n",
    "        lbls = jnp.array(lbls)\n",
    "\n",
    "        recon_x, mu, sigma = model.apply(params, imgs, lbls)\n",
    "        loss = loss_fn(recon_x, imgs, mu, sigma)\n",
    "        test_loss += loss\n",
    "\n",
    "    print(f'epoch: {epoch}| Test Loss: ', test_loss / len(test_loader))\n",
    "\n",
    "    \n",
    "    # # Sample and visualize generated data\n",
    "    # sample = jax.random.normal(jax.random.PRNGKey(1), (1, latent_size))\n",
    "\n",
    "    # for i in range(condition_size):\n",
    "    #     i_number = i * jnp.ones((1,), dtype=int)\n",
    "    #     cond = onehot(i_number, condition_size)\n",
    "    #     inputs = jnp.concatenate((sample, cond), axis=1)\n",
    "\n",
    "    #     # params = model.init(jax.random.PRNGKey(0), inputs, lbls)  # Initialize parameters\n",
    "        \n",
    "    #     # Use model.apply to call the decoder\n",
    "    #     gen = model.apply(params, inputs, cond)[0].reshape((28, 28))\n",
    "    #     plt.matshow(gen)\n",
    "    #     plt.show()\n",
    "\n",
    "# Save the final model parameters after all epochs are completed\n",
    "model_name = \"cvae_jax.pkl\"\n",
    "with open(model_name, \"wb\") as f:\n",
    "    pickle.dump(params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "de85ca09-1713-4d28-aa65-ee8303536d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the saved model parameters\n",
    "model_name = \"cvae_jax.pkl\"\n",
    "with open(model_name, \"rb\") as f:\n",
    "    params = pickle.load(f)\n",
    "\n",
    "# Generate a random sample from the latent space\n",
    "key = jax.random.PRNGKey(0) # change the random key to generate different image\n",
    "sample = jax.random.normal(key, (1, latent_size))  # Shape (1, latent_size)\n",
    "\n",
    "# Define the label to condition on\n",
    "i = 8\n",
    "i_array = jnp.array([i])  # Wrap `i` in an array\n",
    "i_onehot = onehot(i_array, condition_size)  # Use the `onehot` function\n",
    "\n",
    "# Generate the output with the decode method\n",
    "gen_image = model.apply(params, sample, i_onehot, method=model.decode).reshape(28, 28)\n",
    "\n",
    "# Visualize the generated image\n",
    "plt.imshow(gen_image, cmap='gray', interpolation='bilinear')\n",
    "plt.gcf().set_size_inches(2, 2)  # Adjust display size\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b649835-c197-4f6c-9b4e-ff21361ea7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vae_env)",
   "language": "python",
   "name": "vae_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
